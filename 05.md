## linux shell 脚本攻略

## 5

#### 5.1 wget

- wget

		
		wget URL
		
		下载网页或远程文件
		
		可以指定多个URL
		
		wget url1 url2...
		
		
		工作原理
			
			通常下载的文件名 与 URL中文件名 会保持一致 
			
			下载日志 或 进度写入stdout
			
		
		-O 指定输出文件名
		
		-o 指定日志文件  指定日志文件后 就不会打印到stdout
		
		-t 指定重试次数
		
		--limit-rate  限速
		
		-c 断点续传
		
		
		复制整个网站
		
			wget --mirror --convert-links exampledomain.com
			
			或
			
			wget -r -N -l -k DEPTH URL
			
			-l   页面层级
			
			-r   递归  recursive
			
			-N   表示使用文件的时间戳
			
			-k  --convert-links 指示wget页面的链接地址转换为本地地址
			
			--user
			
			--password
			
		
		lynx 是基于命令的web浏览器
		
			lynx URL -dump > text.txt
		 
		 		将网页内容以ASCII码的形式存储的文本
		
		
	
- curl

		
		支持HTTP HTTPS FTP协议
		
		支持POST cookie 认证
		
		curl通常下载文件输出到stdout 将进度信息输出到stderr
		
		--silent   不显示进度信息
		
		curl url
		
		curl url --silent
		
		curl url --silent -O 
		
			-O 表明将下载数据写入文件 而非标准输出中
			
		curl url --progress
		
		
		工作原理
		
			curl url --silent -O new_filename
			
		断点续传
		
			curl url/file -C offset
			
			偏移量是字节为单位的整数
			
			curl -C - url
			
		设置参照页 referer 
		
			curl --referer referer_url target_url
			
		设置cookie
		
			curl url --cookie "cookie-value"
		
		设置ua user-agent
		
			curl url --user-agent "useragent-value"
		
		
		设置头部信息
		
			curl -H 
			
			curl -H "Host: www.google.com" -H "Accept-language: en" url
			
		设置带宽
		
			curl url --limit-rate 20k
			
		设置最大下载量
		
			curl url --max-filesize bytes
		
		设置认证
		
			curl -u user:pass url
			
			
		只打印响应头部消息
		
			response header
			
			设置 -I 或 --head 就可以了
			
			curl -I url
		
	
- 图片抓取器 image crawler

					
					
					
					
					
					
					
	
	
- POST方式发送网页并读取响应

			
		curl URL -d "var1=value1&var2=value2"
		
		-d 表示以POST方式 
		
		
		wget URL --post-data "string"  提交数据
		
		wget url --post-data "user=killsos&passwd=1234"
		
		
			
			
			
			
	